import time

import torch
from sklearn.metrics import accuracy_score, precision_score
from kge.job import EvaluationJob
from kge.util.sampler import TripleClassificationSampler

class TripleClassificationJob(EvaluationJob):
    """Triple classification evaluation protocol.

    Testing model's ability to discriminate between true and false triples based on scores. First, negative (corrupted)
    triples are generated by randomly corrupting each triple in the validation and test data. Then the scores for each
    triple, produced by the model to evaluate, is retrieved. Afterwards a threshold is determined for each relation.
    The best threshold for every relation is determined by maximizing the accuracy on validation data. The unseen
    triples from the train data will then be predicted as True if the score is higher than the threshold of the
    respective relation. The metrics include accuracy and precision on test data. If necessary the accuracy/precision
    per relation can be returned as well.
    """

    def __init__(self, config, dataset, parent_job, model):
        super().__init__(config, dataset, parent_job, model)
        self.valid_data_is_prepared = False

    def _prepare(self):
        """Prepare the corrupted validation and test data.

        The triples are corrupted only for the first evaluated epoch. Afterwards is_prepared is set to true to make sure
        that every epoch is evaluated on the same data. For model selection, the thresholds are found for validation
        data and the accuracy on validation data is used. For testing the thresholds are found for validation data and
        evaluated on test data.
        """

        if self.valid_data_is_prepared:
            return

        self.config.log("Generate data with corrupted and true triples...")

        if self.eval_data == "test":
            self.triples_valid, self.valid_labels, self.rel_valid_labels = TripleClassificationSampler.sample(self, self.dataset.valid)
            self.triples_test, self.test_labels, self.rel_test_labels = TripleClassificationSampler.sample(self, self.dataset.test)
        else:
            self.triples_valid, self.valid_labels, self.rel_valid_label = TripleClassificationSampler.sample(self, self.dataset.valid)
            self.triples_test, self.test_labels, self.rel_test_labels = TripleClassificationSampler.sample(self, self.dataset.valid)

        # let the model add some hooks, if it wants to do so
        self.model.prepare_job(self)
        self.valid_data_is_prepared = True

    def run(self):
        """Runs the triple classification job."""

        self.config.log("Starting triple classification...")
        self._prepare()

        was_training = self.model.training
        self.model.eval()

        epoch_time = -time.time()

        # Get scores and scores per relation for the corrupted valid data
        self.config.log("Compute scores for validation and test datasets...")
        s_valid, p_valid, o_valid = self.triples_valid[:, 0], self.triples_valid[:, 1], self.triples_valid[:, 2]
        valid_scores = self.model.score_spo(s_valid, p_valid, o_valid)
        rel_valid_scores = {int(r): valid_scores[(p_valid == r).nonzero(),:] for r in p_valid.unique()}

        # Get scores and scores per relation for the corrupted test data
        s_test, p_test, o_test = self.triples_test[:, 0], self.triples_test[:, 1], self.triples_test[:, 2]
        test_scores = self.model.score_spo(s_test, p_test, o_test)
        rel_test_scores = {int(r): test_scores[(p_test == r).nonzero(),:] for r in p_test.unique()}

        # Find the best thresholds for every relation on validation data
        self.config.log("Learning thresholds on validation data.")
        rel_thresholds = self.findThresholds(p_valid, valid_scores, rel_valid_scores, self.valid_labels, self.triples_valid)

        # Make prediction for the specified evaluation data
        self.config.log("Evaluating on {} data.".format(self.eval_data))
        rel_predictions, not_in_eval = self.predict(rel_thresholds, rel_test_scores, p_valid, p_test)

        # Compute Metrics
        self.config.log("Classification results:")
        metrics = self._compute_metrics(self.test_labels, self.rel_test_labels, rel_predictions, p_test, not_in_eval)

        epoch_time += time.time()
        # compute trace
        trace_entry = dict(
            type="triple_classification",
            scope="epoch",
            data_thresholds="Valid",
            size_threshold_data = len(self.triples_valid),
            data_evaluate=self.eval_data,
            size_data_evaluate=len(self.triples_test),
            epoch=self.epoch,
            size=2*len(self.dataset.valid),
            epoch_time=epoch_time,
            **metrics,
        )
        for f in self.post_epoch_trace_hooks:
            f(self, trace_entry)

        # if validation metric is not present, try to compute it
        metric_name = self.config.get("valid.metric")
        if metric_name not in trace_entry:
            trace_entry[metric_name] = eval(
                self.config.get("valid.metric_expr"),
                None,
                {"config": self.config, **trace_entry},
            )

        # write out trace
        trace_entry = self.trace(**trace_entry, echo=True, echo_prefix="  ", log=True)

        # reset model and return metrics
        if was_training:
            self.model.train()
        self.config.log("Finished evaluating on " + self.eval_data + " data.")

        return trace_entry

    def findThresholds(self, p, valid_scores, rel_scores, valid_labels, valid_data):
        """Find the best thresholds per relation by maximizing accuracy on validation data.

        The thresholds are found for every relation by maximizing the accuracy on the validation data. For a given
        relation, if the scores of all triple in the relation are sorted, the perfect threshold is always a cut between
        two of the scores. This means, that multiple possible values can be defined as thresholds and give the highest
        accuracy. To evaluate only as many possible thresholds as really necessary, the scores themselves are considered
        as possible thresholds. This allows for a fast implementation.

        Args:
            p: 1-D tensor containing the relations of the corrupted validation dataset.
            valid_scores: 2-D tensor containing the scores of all corrupted validation triples.
            rel_scores: Dictionary containing the scores of the triples in a relation.
            valid_labels: 1-D tensor containing the labels of all corrupted validation triples.
            valid_data: Dataset used. Should be the corrupted validation dataset.

        Returns:
            rel_thresholds: Dictionary with thresholds per relation {relation: thresholds}.
                            E.g.: {1: tensor(-2.0843, grad_fn=<SelectBackward>)}
        """

        # Initialize accuracies and thresholds
        rel_accuracies = {int(r): -1 for r in p.unique()}
        rel_thresholds = {int(r): 0 for r in p.unique()}

        # Change the valid scores from a 2D to a 1D tensor
        valid_scores = torch.as_tensor([float(valid_scores[i]) for i in range(len(valid_scores))]).to(self.device)

        for r in p.unique():
            current_rel = (valid_data[:, 1] == r) # 0-1 vector for indexing triples of the current relation
            true_labels = valid_labels[current_rel.nonzero()].type(torch.int)

            # valid_scores[current_rel.nonzero()] and rel_scores[int(r)] both contain the scores of the current
            # relation. In the comparison, every score is evaluated as possible threshold against all scores.
            predictions = (valid_scores[current_rel.nonzero()] >= rel_scores[int(r)]).type(torch.int)

            accuracy = [int(((true_labels==predictions[i]).sum(dim=0)))/len(true_labels) for i in range(len(rel_scores[int(r)]))]
            rel_accuracies[int(r)] = max(accuracy)

            # Choose the smallest score of the ones which give the maximum accuracy as threshold to stay consistent.
            rel_thresholds[int(r)] = min(rel_scores[int(r)][list(filter(lambda x: accuracy[x] == max(accuracy), range(len(accuracy))))])[0,0]

#     # Alternative implementation: Search for best threshold in an interval
#     # Following https://github.com/siddharth-agrawal/Neural-Tensor-Network/blob/master/neuralTensorNetwork.py or
#     # https://github.com/dddoss/tensorflow-socher-ntn/blob/master/code/ntn_eval.py (reimplemented Socher et al. 2013)
#
#     # Initialize accuracies, thresholds and interval
#     min_score = valid_scores.min()
#     max_score = valid_scores.max()
#
#     rel_accuracies = {int(r): -1 for r in p.unique()}
#     rel_thresholds = {int(r): min_score for r in p.unique()}
#
#     score = min_score
#
#     # Original implementation uses an interval of 0.01, implemented for NTN model. In general the interval should
#     # depend on the range of the score values of the model and be at least as large as teh smallest distance between
#     # two of the sorted scores
#     interval = 0.01
#     valid_scores = torch.as_tensor([float(valid_scores[i]) for i in range(len(valid_scores))]).to(self.device)
#
#     while(score<=max_score):
#         for r in p.unique():
#             #Predict
#             current_rel = (valid_data[:, 1] == r)
#             true_labels = valid_labels[current_rel.nonzero()].type(torch.int)
#             predictions = (valid_scores[current_rel.nonzero()] >= score).type(torch.int)
#             accuracy = int(((true_labels==predictions).sum(dim=0)))/len(true_labels)
#
#             if accuracy > rel_accuracies[int(r)]:
#                 rel_accuracies[int(r)] = accuracy
#                 rel_thresholds[int(r)] = score.clone()
#
#         score += interval

        return rel_thresholds

    def predict(self, rel_thresholds, rel_scores, p_valid, p_test):
        """Makes predictions on evaluation/test data.

        Parameters:
            rel_thresholds: Dictionary with relation thresholds.
            rel_scores: Dictionary with scores of triples in each relation:
                        E.g. relation with four triples in it:, e.g. {1: [-2, 1, 2, 4]}.

        Returns:
            rel_predictions: Dictionary with predictions for the triples in a relation, e.g. {1: [0, 0, 1, 1]}.
            not_in_eval: List with relations that are in the test data, but not in the validation data.
        """

        rel_predictions = {int(r): torch.as_tensor([0]*len(rel_scores[int(r)])).to(self.device) for r in p_test.unique()}

        # Set variable for relations which are not in valid data, but in test data
        not_in_eval = []
        for r in p_test.unique():
            if r in p_valid.unique(): # Check if relation which is in valid data also is in test data
                # Predict
                rel_predictions[int(r)] = rel_scores[int(r)][:, 0, 0] >= rel_thresholds[int(r)]
            else: not_in_eval.append(r)

        return rel_predictions, not_in_eval

    def _compute_metrics(self, test_labels, rel_test_labels, rel_predictions, p_test, not_in_eval):
        """Computes accuracy and precision metrics of predictions.

        Returns:
            metrics: dictionary with the specified metrics accuracy and precision as keys. If specified, metrics per
            relation are stored as dictionaries in the dictionary.
            E.g.: {accuracy: 0.9
                   accuracy_per_relation:
                        {relation 1: 0.8}
                        {relation 2: 0.9}
                    }
        """
        metrics = {}

        # Create a list for all predicted labels, matching the shape of test_labels
        pred_list = torch.tensor([i
                     for r in p_test.unique()
                     for i in rel_predictions[int(r)]], dtype=torch.int64)

        metrics["accuracy"] = float(accuracy_score(test_labels, pred_list))
        metrics["precision"] = float(precision_score(test_labels, pred_list))

        if self.config.get("eval.metrics_per.relation"):
            precision_per_r = {}
            accuracy_per_r = {}
            for r in p_test.unique():
                    precision_per_r[str(self.dataset.relations[int(r)])] = \
                        float(precision_score(rel_test_labels[int(r)], rel_predictions[int(r)]))
                    accuracy_per_r[str(self.dataset.relations[int(r)])] = \
                        float(accuracy_score(rel_test_labels[int(r)], rel_predictions[int(r)]))

            metrics["accuracy_per_relation"] = accuracy_per_r

            metrics["precision_per_relation"] = precision_per_r


        metrics["untested relations due to missing in evaluation data"] = len(not_in_eval)

        return metrics