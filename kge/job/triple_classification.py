import time

import torch
from sklearn.metrics import accuracy_score, precision_score
from kge.job import EvaluationJob
from kge.util.sampler import TripleClassificationSampler


class TripleClassificationJob(EvaluationJob):
    """Triple classification evaluation protocol.

    Testing a model's ability to classify true and false triples based on
    thresholding scores. First, negative (corrupted) triples are generated by
    randomly corrupting each triple in the validation and test data. Then the
    scores for each triple, produced by the model to evaluate, is retrieved.
    Afterwards a threshold is determined for each relation. The best threshold
    for every relation is determined by maximizing the accuracy on validation
    data. The unseen triples from the train data will then be predicted as True
    if the score is higher than the threshold of the respective relation. The
    metrics include accuracy and precision on test data. If necessary the
    accuracy/precision per relation can be returned as well.
    """

    def __init__(self, config, dataset, parent_job, model):
        super().__init__(config, dataset, parent_job, model)
        self.valid_data_is_prepared = False
        self.triple_classification_sampler = TripleClassificationSampler(config, "config_key", dataset)

    def _prepare(self):
        """Prepare the corrupted validation and test data.

        The triples are corrupted only for the first evaluated epoch. Afterwards
        is_prepared is set to true to make sure that every epoch is evaluated on
        the same data. For model selection, the thresholds are found for validation
        data and the accuracy on validation data is used. For testing the
        thresholds are found for validation data and evaluated on test data.
        """

        if self.valid_data_is_prepared:
            return

        self.config.log("Generate data with corrupted and true triples...")

        if self.eval_split == "test":
            (
                self.tune_data,
                self.tune_labels,
                self.rel_tune_labels,
            ) = self.triple_classification_sampler.sample(self.dataset.split('valid'))
            (
                self.eval_data,
                self.eval_labels,
                self.rel_eval_labels,
            ) = self.triple_classification_sampler.sample(self.dataset.split('test'))
        else:
            (
                self.tune_data,
                self.tune_labels,
                self.rel_tune_label,
            ) = self.triple_classification_sampler.sample(self.dataset.split('valid'))
            (
                self.eval_data,
                self.eval_labels,
                self.rel_eval_labels,
            ) = self.triple_classification_sampler.sample(self.dataset.split('valid'))

        # let the model add some hooks, if it wants to do so
        self.model.prepare_job(self)
        self.valid_data_is_prepared = True

    def run(self):
        """Runs the triple classification job."""

        self.config.log("Starting triple classification...")
        self._prepare()

        was_training = self.model.training
        self.model.eval()

        epoch_time = -time.time()

        # Get scores and scores per relation for the corrupted valid data
        self.config.log("Compute scores for tune and eval datasets...")
        s_tune, p_tune, o_tune = (
            self.tune_data[:, 0],
            self.tune_data[:, 1],
            self.tune_data[:, 2],
        )
        p_tune_unique = p_tune.unique()
        tune_scores = self.model.score_spo(s_tune, p_tune, o_tune)
        rel_tune_scores = {
            r: tune_scores[(p_tune == r)] for r in p_tune_unique
        }

        # Get scores and scores per relation for the corrupted test data
        s_eval, p_eval, o_eval = (
            self.eval_data[:, 0],
            self.eval_data[:, 1],
            self.eval_data[:, 2],
        )
        p_eval_unique = p_eval.unique()
        eval_scores = self.model.score_spo(s_eval, p_eval, o_eval)

        # Find the best thresholds for every relation on validation data
        self.config.log("Tuning thresholds.")
        rel_thresholds = self.findThresholds(
            p_tune_unique,
            tune_scores,
        )

        # Make prediction for the specified evaluation data
        self.config.log("Evaluating on {} data.".format(self.eval_split))
        rel_predictions, not_in_eval = self.predict(
            eval_scores, rel_thresholds, p_tune_unique, p_eval_unique
        )

        # Compute Metrics
        self.config.log("Classification results:")
        metrics = self._compute_metrics(
            self.eval_labels, self.rel_eval_labels, rel_predictions, p_eval, not_in_eval
        )

        epoch_time += time.time()
        # compute trace
        trace_entry = dict(
            type="triple_classification",
            scope="epoch",
            data_thresholds="Valid",
            size_threshold_data=len(self.tune_data),
            data_evaluate=self.eval_split,
            size_data_evaluate=len(self.eval_data),
            epoch=self.epoch,
            size=2 * len(self.dataset.valid),
            epoch_time=epoch_time,
            **metrics,
        )
        for f in self.post_epoch_trace_hooks:
            f(self, trace_entry)

        # if validation metric is not present, try to compute it
        metric_name = self.config.get("valid.metric")
        if metric_name not in trace_entry:
            trace_entry[metric_name] = eval(
                self.config.get("valid.metric_expr"),
                None,
                {"config": self.config, **trace_entry},
            )

        # write out trace
        trace_entry = self.trace(**trace_entry, echo=True, echo_prefix="  ", log=True)

        # reset model and return metrics
        if was_training:
            self.model.train()
        self.config.log("Finished evaluating on " + self.eval_data + " data.")

        return trace_entry

    def findThresholds(
        self, p_tune_unique, tune_scores
    ):
        """Find the best thresholds per relation by maximizing accuracy on
        validation data.

        The thresholds are found for every relation by maximizing the accuracy on
        the validation data. For a given relation, if the scores of all triple in
        the relation are sorted, the perfect threshold is always a cut between two
        of the scores. This means, that multiple possible values can be defined as
        thresholds and give the highest accuracy. To evaluate only as many possible
        thresholds as really necessary, the scores themselves are considered as
        possible thresholds. This allows for a fast implementation.

        Args:

            p_tune: 1-D tensor containing the relations of the corrupted validation
            dataset.

            tune_scores: 2-D tensor containing the scores of all corrupted
            validation triples.

            rel_tune_scores: Dictionary containing the scores of the triples in a
            relation.

            tune_thresh_labels: 1-D tensor containing the labels of all corrupted
            tuning triples.

            tune_data: Dataset used. Should be the corrupted validation dataset.

        Returns:
            rel_thresholds: Dictionary with thresholds per relation
                            {relation: thresholds}.
                            E.g.: {1: tensor(-2.0843, grad_fn=<SelectBackward>)}
        """

        # Initialize accuracies and thresholds
        rel_accuracies = {r: -1 for r in p_tune_unique}
        rel_thresholds = {r: 0 for r in p_tune_unique}

        # Change the valid scores from a 2D to a 1D tensor
        # tune_scores = torch.as_tensor(
        #     [float(tune_scores[i]) for i in range(len(tune_scores))]
        # ).to(self.device)

        for r in p_tune_unique:
            # 0-1 vector for indexing triples of the current relation
            current_rel = (
                self.tune_data[:, 1] == r
            )
            true_labels = self.tune_labels[current_rel]

            # tune_scores[current_rel] and rel_tune_scores[r] both
            # contain the scores of the current relation. In the comparison, every
            # score is evaluated as possible threshold against all scores.
            predictions = (
                tune_scores[current_rel].view(-1, 1) >= tune_scores[current_rel].view(1, -1)
            )

            accuracies = (predictions & true_labels).float().sum(dim=1) / true_labels.size(0)
            rel_accuracies[r] = accuracies.max()

            # Choose the smallest score of the ones which give the maximum
            # accuracy as threshold to stay consistent.
            rel_thresholds[r] = (tune_scores[current_rel][rel_accuracies[r] >= tune_scores[current_rel]]).min()

        return rel_thresholds

    def predict(self, eval_scores, rel_thresholds, p_tune_unique, p_eval_unique):
        """Makes predictions on evaluation/test data.

        Parameters:
            rel_thresholds: Dictionary with relation thresholds.

        Returns:
            rel_predictions: Dictionary with predictions for the triples in a relation, e.g. {1: [0, 0, 1, 1]}.
            not_in_eval: List with relations that are in the test data, but not in the validation data.
        """

        rel_predictions = dict()
        # Set variable for relations which are not in valid data, but in test data
        not_in_eval = []
        for r in p_eval_unique:
            if (
                r in p_tune_unique
            ):  # Check if relation which is in valid data also is in test data
                # Predict
                current_rel = (
                        self.eval_data[:, 1] == r
                )
                rel_predictions[r] = (
                    eval_scores[current_rel] >= rel_thresholds[r]
                )
            else:
                not_in_eval.append(r)

        return rel_predictions, not_in_eval

    def _compute_metrics(
        self, test_labels, rel_test_labels, rel_predictions, p_test, not_in_eval
    ):
        """Computes accuracy and precision metrics of predictions.

        Returns:
            metrics: dictionary with the specified metrics accuracy and precision as keys. If specified, metrics per
            relation are stored as dictionaries in the dictionary.
            E.g.: {accuracy: 0.9
                   accuracy_per_relation:
                        {relation 1: 0.8}
                        {relation 2: 0.9}
                    }
        """
        metrics = {}

        # Create a list for all predicted labels, matching the shape of test_labels
        pred_list = torch.tensor(
            [i for r in p_test.unique() for i in rel_predictions[int(r)]],
            dtype=torch.int64,
        )

        metrics["accuracy"] = float(accuracy_score(test_labels, pred_list))
        metrics["precision"] = float(precision_score(test_labels, pred_list))

        if self.config.get("eval.metrics_per.relation"):
            precision_per_r = {}
            accuracy_per_r = {}
            for r in p_test.unique():
                precision_per_r[str(self.dataset.relations[int(r)])] = float(
                    precision_score(rel_test_labels[int(r)], rel_predictions[int(r)])
                )
                accuracy_per_r[str(self.dataset.relations[int(r)])] = float(
                    accuracy_score(rel_test_labels[int(r)], rel_predictions[int(r)])
                )

            metrics["accuracy_per_relation"] = accuracy_per_r

            metrics["precision_per_relation"] = precision_per_r

        metrics["untested relations due to missing in evaluation data"] = len(
            not_in_eval
        )

        return metrics
